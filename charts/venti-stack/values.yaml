global:
  imageRepository: ghcr.io/kuoss

fluent-bit:
  # https://github.com/fluent/helm-charts/blob/main/charts/fluent-bit/values.yaml
  testFramework:
    enabled: false
  service:
    annotations:
      prometheus.io/path: "/api/v1/metrics/prometheus"
      prometheus.io/port: "2020"
      prometheus.io/scrape: "true"
  tolerations:
    - effect: NoSchedule
      operator: Exists
  luaScripts:
    kube.lua: |
      function kube(tag, timestamp, r)
        local u = os.date("%Y-%m-%dT%H:%M:%SZ")
        local n,p,c = tag:match("^kube_([^%.]+)_([^%.]+)_([^%.]+)$")
        return 1, timestamp, {tag="pod/"..n.."/"..string.sub(u,1,10).."_"..string.sub(u,12,13)..".log", row=u.."["..n.."|"..p.."|"..c.."] "..r["log"]}
      end
    host.lua: |
      function host(tag, timestamp, r)
        local u = os.date("%Y-%m-%dT%H:%M:%SZ")
        return 1, timestamp, {tag="node/"..r["_HOSTNAME"].."/"..string.sub(u,1,10).."_"..string.sub(u,12,13)..".log", row=u.."["..r["_HOSTNAME"].."|"..r["SYSLOG_IDENTIFIER"].."] "..r["MESSAGE"]}
      end
  config:
    customParsers: |
      [PARSER]
          Name              containerd
          Format            regex
          Regex             ^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$
          Time_Key          time
          Time_Format       %Y-%m-%dT%H:%M:%S.%L%z

    inputs: |
      [INPUT]
          Name              tail
          Path              /var/log/containers/*.log
          Exclude_Path      /var/log/containers/*_fluent-bit-*.log
          Tag               kube_<namespace>_<pod>_<container>
          Tag_Regex         (?<pod>[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?<namespace>[^_]+)_(?<container>.+)-
          Parser            containerd
          Mem_Buf_Limit     5MB
          Skip_Long_Lines   On

      [INPUT]
          Name              systemd
          Tag               host.*
          Systemd_Filter    _SYSTEMD_UNIT=containerd.service
          Systemd_Filter    _SYSTEMD_UNIT=etcd.service
          Systemd_Filter    _SYSTEMD_UNIT=kubelet.service
          Read_From_Tail    On

    filters: |
      [FILTER]
          Name              lua
          Match             kube_*
          script            /fluent-bit/scripts/kube.lua
          call              kube

      [FILTER]
          Name              lua
          Match             host.*
          script            /fluent-bit/scripts/host.lua
          call              host

      [FILTER]
          Name              rewrite_tag
          Match_regex       ^(kube|host)
          Rule              $tag .* $tag false
          Emitter_Name      re_emitted

      [FILTER]
          Name              modify
          Match_regex       ^(pod|node)
          Remove            tag

    outputs: |
      [OUTPUT]
          Name              forward
          Match_regex       ^(pod|node)
          Host              {{ .Release.Name }}-lethe
          Port              24224

prometheus:
  enabled: true
  # https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml
  alertmanager:
    persistence:
      enabled: false
  prometheus-node-exporter:
    extraArgs:
      - --collector.systemd
    extraHostVolumeMounts:
      - name: systemd-socket
        mountPath: /var/run/dbus/system_bus_socket
        hostPath: /var/run/dbus/system_bus_socket
        readOnly: true
        mountPropagation: None
    nameOverride: "node-exporter"
  prometheus-pushgateway:
    enabled: false

eventrouter:
  enabled: true
  nodeSelector: {}
  tolerations: []
  resources:
    limits:
      cpu: 100m
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 100Mi

lethe:
  automountServiceAccountToken: false
  serviceAccount:
    create: true
    annotations: {}
    name: ""

  nodeSelector: {}
  hostAliases: []
  dnsConfig: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: []

  service:
    enabled: true
    annotations:
      prometheus.io/path: "/api/v1/metrics/prometheus"
      prometheus.io/port: "2020"
      prometheus.io/scrape: "true"
    labels: {}
    clusterIP: ""
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    sessionAffinity: None
    type: ClusterIP

  ingress:
    enabled: false
    annotations: {}
    extraLabels: {}
    hosts: []
    path: /
    pathType: Prefix
    extraPaths: []
    tls: []

  extraConfigmapLabels: {}
  persistentVolume:
    enabled: true
    accessModes:
      - ReadWriteOnce
    labels: {}
    annotations: {}
    existingClaim: ""
    mountPath: /data
    size: 8Gi
    # storageClass: "-"

  emptyDir:
    sizeLimit: ""

  retention:
    size: 7g
    time: 15d

  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 1000m
      memory: 1000Mi

venti:
  enabled: true
  config:
    global:
      ginMode: release
      logLevel: info
    alerting:
      evaluationInterval: 5s
      globalLabels:
        addChartLabel: true
        extraLabels: {}
    alertRules:
      defaultGroups:
        # kubernetes - https://github.com/samber/awesome-prometheus-alerts/tree/master/dist/rules/kubernetes
        # node       - https://github.com/samber/awesome-prometheus-alerts/tree/master/dist/rules/host-and-hardware
        # prometheus - https://github.com/samber/awesome-prometheus-alerts/tree/master/dist/rules/prometheus-self-monitoring
        enabled: true
        kubernetes:
          enabled: true
          rules:
            k01:
              alert: k01-KubernetesNodeNotReady
              enabled: true
              expr: kube_node_status_condition{condition="Ready",status="true"} == 0
              for: 10m
              severity: critical
              summary: (node {{ $labels.node }}) Node {{ $labels.node }} has been unready for a long time ({{ $value }})
            k02:
              alert: k02-KubernetesNodeMemoryPressure
              enabled: true
              expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
              for: 2m
              severity: critical
              summary: (node {{ $labels.node }}) Node {{ $labels.node }} has MemoryPressure condition ({{ $value }})
            k03:
              alert: k03-KubernetesNodeDiskPressure
              enabled: true
              expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
              for: 2m
              severity: critical
              summary: (node {{ $labels.node }}) Node {{ $labels.node }} has DiskPressure condition ({{ $value }})
            k04:
              alert: k04-KubernetesNodeNetworkUnavailable
              enabled: true
              expr: kube_node_status_condition{condition="NetworkUnavailable",status="true"} == 1
              for: 2m
              severity: critical
              summary: (instance {{ $labels.instance }}) Node {{ $labels.node }} has NetworkUnavailable condition ({{ $value }})
            k05:
              alert: k05-KubernetesNodeOutOfPodCapacity
              enabled: true
              expr: sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid, instance) group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node) (kube_node_status_allocatable{resource="pods"}) * 100 > 90
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Node {{ $labels.node }} is out of pod capacity ({{ $value }})
            k06:
              alert: k06-KubernetesContainerOomKiller
              enabled: true
              expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
              for: 0m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.pod }}{{ $labels.container }}) Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes. ({{ $value }})
            k07:
              alert: k07-KubernetesJobFailed
              enabled: true
              expr: kube_job_status_failed > 0
              for: 0m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.job_name }}) Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete ({{ $value }})
            k08:
              alert: k08-KubernetesJobNotStarting
              enabled: true
              expr: kube_job_status_active == 0 and kube_job_status_failed == 0 and kube_job_status_succeeded == 0 and (time() - kube_job_status_start_time) > 600
              for: 0m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.job_name }}) Job {{ $labels.namespace }}/{{ $labels.job_name }} did not start for 10 minutes ({{ $value }})
            k09:
              alert: k09-KubernetesCronjobSuspended
              enabled: true
              expr: kube_cronjob_spec_suspend != 0
              for: 0m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.cronjob }}) CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended ({{ $value }})
            k10:
              alert: k10-KubernetesPersistentvolumeclaimPending
              enabled: true
              expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
              for: 2m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}) PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending ({{ $value }})
            k11:
              alert: k11-KubernetesVolumeOutOfDiskSpace
              enabled: true
              expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Volume is almost full (< 10% left) ({{ $value }})
            k12:
              alert: k12-KubernetesVolumeFullInFourDays
              enabled: false
              expr: predict_linear(kubelet_volume_stats_available_bytes[6h:5m], 4 * 24 * 3600) < 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Volume under {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available. ({{ $value }})
            k13:
              alert: k13-KubernetesPersistentvolumeError
              enabled: true
              expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0
              for: 0m
              severity: critical
              summary: ({{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}) Persistent volume {{ $labels.persistentvolume }} is in bad state ({{ $value }})
            k14:
              alert: k14-KubernetesStatefulsetDown
              enabled: true
              expr: kube_statefulset_replicas != kube_statefulset_status_replicas_ready > 0
              for: 1m
              severity: critical
              summary: ({{ $labels.namespace }}/{{ $labels.statefulset }}) StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} went down ({{ $value }})
            k15:
              alert: k15-KubernetesHpaScaleInability
              enabled: true
              expr: (kube_horizontalpodautoscaler_spec_max_replicas - kube_horizontalpodautoscaler_status_desired_replicas) * on (horizontalpodautoscaler,namespace) (kube_horizontalpodautoscaler_status_condition{condition="ScalingLimited", status="true"} == 1) == 0
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} is unable to scale ({{ $value }})
            k16:
              alert: k16-KubernetesHpaMetricsUnavailability
              enabled: true
              expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="ScalingActive"} == 1
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} is unable to collect metrics ({{ $value }})
            k17:
              alert: k17-KubernetesHpaScaleMaximum
              enabled: true
              expr: (kube_horizontalpodautoscaler_status_desired_replicas >= kube_horizontalpodautoscaler_spec_max_replicas) and (kube_horizontalpodautoscaler_spec_max_replicas > 1) and (kube_horizontalpodautoscaler_spec_min_replicas != kube_horizontalpodautoscaler_spec_max_replicas)
              for: 2m
              severity: info
              summary: (instance {{ $labels.instance }}) HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} has hit maximum number of desired pods ({{ $value }})
            k18:
              alert: k18-KubernetesHpaUnderutilized
              enabled: true
              expr: max(quantile_over_time(0.5, kube_horizontalpodautoscaler_status_desired_replicas[1d]) == kube_horizontalpodautoscaler_spec_min_replicas) by (horizontalpodautoscaler) > 3
              for: 0m
              severity: info
              summary: (instance {{ $labels.instance }}) HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} is constantly at minimum replicas for 50% of the time. Potential cost saving here. ({{ $value }})
            k19:
              alert: k19-KubernetesPodNotHealthy
              enabled: true
              expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0
              for: 15m
              severity: critical
              summary: ({{ $labels.namespace }}/{{ $labels.pod }}) Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-running state for longer than 15 minutes. ({{ $value }})
            k20:
              alert: k20-KubernetesPodCrashLooping
              enabled: true
              expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
              for: 2m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.pod }}) Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping ({{ $value }})
            k21:
              alert: k21-KubernetesReplicasetReplicasMismatch
              enabled: true
              expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
              for: 10m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.replicaset }}) ReplicaSet {{ $labels.namespace }}/{{ $labels.replicaset }} replicas mismatch ({{ $value }})
            k22:
              alert: k22-KubernetesDeploymentReplicasMismatch
              enabled: true
              expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
              for: 10m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.deployment }}) Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch ({{ $value }})
            k23:
              alert: k23-KubernetesStatefulsetReplicasMismatch
              enabled: true
              expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
              for: 10m
              severity: warning
              summary: (instance {{ $labels.instance }}) StatefulSet does not match the expected number of replicas. ({{ $value }})
            k24:
              alert: k24-KubernetesDeploymentGenerationMismatch
              enabled: true
              expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
              for: 10m
              severity: critical
              summary: ({{ $labels.namespace }}/{{ $labels.deployment }}) Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has failed but has not been rolled back. ({{ $value }})
            k25:
              alert: k25-KubernetesStatefulsetGenerationMismatch
              enabled: true
              expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
              for: 10m
              severity: critical
              summary: ({{ $labels.namespace }}/{{ $labels.statefulset }}) StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has failed but has not been rolled back. ({{ $value }})
            k26:
              alert: k26-KubernetesStatefulsetUpdateNotRolledOut
              enabled: true
              expr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
              for: 10m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.statefulset }}) StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out. ({{ $value }})
            k27:
              alert: k27-KubernetesDaemonsetRolloutStuck
              enabled: true
              expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
              for: 10m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.daemonset }}) Some Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled or not ready ({{ $value }})
            k28:
              alert: k28-KubernetesDaemonsetMisscheduled
              enabled: true
              expr: kube_daemonset_status_number_misscheduled > 0
              for: 1m
              severity: critical
              summary: ({{ $labels.namespace }}/{{ $labels.daemonset }}) Some Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run ({{ $value }})
            k29:
              alert: k29-KubernetesCronjobTooLong
              enabled: true
              expr: time() - kube_cronjob_next_schedule_time > 3600
              for: 0m
              severity: warning
              summary: ({{ $labels.namespace }}/{{ $labels.cronjob }}) CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete. ({{ $value }})
            k30:
              alert: k30-KubernetesJobSlowCompletion
              enabled: true
              expr: kube_job_spec_completions - kube_job_status_succeeded - kube_job_status_failed > 0
              for: 12h
              severity: critical
              summary: ({{ $labels.namespace }}/{{ $labels.job_name }}) Kubernetes Job {{ $labels.namespace }}/{{ $labels.job_name }} did not complete in time. ({{ $value }})
            k31:
              alert: k31-KubernetesApiServerErrors
              enabled: true
              expr: sum(rate(apiserver_request_total{job="apiserver",code=~"(?:5..)"}[1m])) by (instance, job) / sum(rate(apiserver_request_total{job="apiserver"}[1m])) by (instance, job) * 100 > 3
              for: 2m
              severity: critical
              summary: (instance {{ $labels.instance }}) Kubernetes API server is experiencing high error rate ({{ $value }})
            k32:
              alert: k32-KubernetesApiClientErrors
              enabled: true
              expr: (sum(rate(rest_client_requests_total{code=~"(4|5).."}[1m])) by (instance, job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) * 100 > 1
              for: 2m
              severity: critical
              summary: (instance {{ $labels.instance }}) Kubernetes API client is experiencing high error rate ({{ $value }})
            k33:
              alert: k33-KubernetesClientCertificateExpiresNextWeek
              enabled: true
              expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) A client certificate used to authenticate to the apiserver is expiring next week. ({{ $value }})
            k34:
              alert: k34-KubernetesClientCertificateExpiresSoon
              enabled: true
              expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 24*60*60
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours. ({{ $value }})
            k35:
              alert: k35-KubernetesApiServerLatency
              enabled: true
              expr: histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{verb!~"(?:CONNECT|WATCHLIST|WATCH|PROXY)"} [10m])) WITHOUT (subresource)) > 1
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Kubernetes API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}. ({{ $value }})
        node:
          enabled: true
          rules:
            n01:
              alert: n01-NodeOutOfMemory
              enabled: true
              expr: (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes < .10)
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Node memory is filling up (< 10% left) ({{ $value }})
            n02:
              alert: n02-NodeMemoryUnderMemoryPressure
              enabled: true
              expr: (rate(node_vmstat_pgmajfault[5m]) > 1000)
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) The node is under heavy memory pressure. High rate of loading memory pages from disk. ({{ $value }})
            n03:
              alert: n03-NodeMemoryIsUnderutilized
              enabled: true
              expr: min_over_time(node_memory_MemFree_bytes[1w]) > node_memory_MemTotal_bytes * .8
              for: 0m
              severity: info
              summary: (instance {{ $labels.instance }}) Node memory usage is < 20% for 1 week. Consider reducing memory space. (instance {{ $labels.instance }}) ({{ $value }})
            n04:
              alert: n04-NodeUnusualNetworkThroughputIn
              enabled: true
              expr: ((rate(node_network_receive_bytes_total[5m]) / on(instance, device) node_network_speed_bytes) > .80)
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) Host receive bandwidth is high (>80%). ({{ $value }})
            n05:
              alert: n05-NodeUnusualNetworkThroughputOut
              enabled: true
              expr: ((rate(node_network_transmit_bytes_total[5m]) / on(instance, device) node_network_speed_bytes) > .80)
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) Host transmit bandwidth is high (>80%) ({{ $value }})
            n06:
              alert: n06-NodeUnusualDiskReadRate
              enabled: true
              expr: (rate(node_disk_io_time_seconds_total[5m]) > .80)
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) Disk is too busy (IO wait > 80%) ({{ $value }})
            n07:
              alert: n07-NodeOutOfDiskSpace
              enabled: true
              expr: (node_filesystem_avail_bytes{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"} / node_filesystem_size_bytes < .10 and on (instance, device, mountpoint) node_filesystem_readonly == 0)
              for: 2m
              severity: critical
              summary: (instance {{ $labels.instance }}) Disk is almost full (< 10% left) ({{ $value }})
            n08:
              alert: n08-NodeDiskMayFillIn24Hours
              enabled: false
              expr: predict_linear(node_filesystem_avail_bytes{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"}[1h], 86400) <= 0 and node_filesystem_avail_bytes > 0
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Filesystem will likely run out of space within the next 24 hours. ({{ $value }})
            n09:
              alert: n09-NodeOutOfInodes
              enabled: true
              expr: (node_filesystem_files_free / node_filesystem_files < .10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0)
              for: 2m
              severity: critical
              summary: (instance {{ $labels.instance }}) Disk is almost running out of available inodes (< 10% left) ({{ $value }})
            n10:
              alert: n10-NodeFilesystemDeviceError
              enabled: true
              expr: node_filesystem_device_error{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"} == 1
              for: 2m
              severity: critical
              summary: (instance {{ $labels.instance }}) Error stat-ing the {{ $labels.mountpoint }} filesystem ({{ $value }})
            n11:
              alert: n11-NodeInodesMayFillIn24Hours
              enabled: false
              expr: predict_linear(node_filesystem_files_free{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"}[1h], 86400) <= 0 and node_filesystem_files_free > 0
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Filesystem will likely run out of inodes within the next 24 hours at current write rate ({{ $value }})
            n12:
              alert: n12-NodeUnusualDiskReadLatency
              enabled: true
              expr: (rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0)
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Disk latency is growing (read operations > 100ms) ({{ $value }})
            n13:
              alert: n13-NodeUnusualDiskWriteLatency
              enabled: true
              expr: (rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0)
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Disk latency is growing (write operations > 100ms) ({{ $value }})
            n14:
              alert: n14-NodeHighCpuLoad
              enabled: true
              expr: (avg by (instance) (rate(node_cpu_seconds_total{mode!="idle"}[2m]))) > .80
              for: 10m
              severity: warning
              summary: (instance {{ $labels.instance }}) CPU load is > 80% ({{ $value }})
            n15:
              alert: n15-NodeCpuIsUnderutilized
              enabled: true
              expr: (min by (instance) (rate(node_cpu_seconds_total{mode="idle"}[1h]))) > 0.8
              for: 1w
              severity: info
              summary: (instance {{ $labels.instance }}) CPU load has been < 20% for 1 week. Consider reducing the number of CPUs. ({{ $value }})
            n16:
              alert: n16-NodeCpuStealNoisyNeighbor
              enabled: true
              expr: avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit. ({{ $value }})
            n17:
              alert: n17-NodeCpuHighIowait
              enabled: true
              expr: avg by (instance) (rate(node_cpu_seconds_total{mode="iowait"}[5m])) > .10
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) CPU iowait > 10%. Your CPU is idling waiting for storage to respond. ({{ $value }})
            n18:
              alert: n18-NodeUnusualDiskIo
              enabled: true
              expr: rate(node_disk_io_time_seconds_total[5m]) > 0.8
              for: 5m
              severity: warning
              summary: (instance {{ $labels.instance }}) Disk usage >80%. Check storage for issues or increase IOPS capabilities. Check storage for issues. ({{ $value }})
            n19:
              alert: n19-NodeContextSwitchingHigh
              enabled: true
              expr: (rate(node_context_switches_total[15m])/count without(mode,cpu) (node_cpu_seconds_total{mode="idle"})) / (rate(node_context_switches_total[1d])/count without(mode,cpu) (node_cpu_seconds_total{mode="idle"})) > 2
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) Context switching is growing on the node (twice the daily average during the last 15m) ({{ $value }})
            n20:
              alert: n20-NodeSwapIsFillingUp
              enabled: true
              expr: ((1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80)
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Swap is filling up (>80%) ({{ $value }})
            n21:
              alert: n21-NodeSystemdServiceCrashed
              enabled: true
              expr: (node_systemd_unit_state{state="failed"} == 1)
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) systemd service crashed ({{ $value }})
            n22:
              alert: n22-NodePhysicalComponentTooHot
              enabled: true
              expr: node_hwmon_temp_celsius > node_hwmon_temp_max_celsius
              for: 5m
              severity: warning
              summary: (instance {{ $labels.instance }}) Physical hardware component too hot ({{ $value }})
            n23:
              alert: n23-NodeNodeOvertemperatureAlarm
              enabled: true
              expr: ((node_hwmon_temp_crit_alarm_celsius == 1) or (node_hwmon_temp_alarm == 1))
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Physical node temperature alarm triggered ({{ $value }})
            n24:
              alert: n24-NodeSoftwareRaidInsufficientDrives
              enabled: true
              expr: ((node_md_disks_required - on(device, instance) node_md_disks{state="active"}) > 0)
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) MD RAID array {{ $labels.device }} on {{ $labels.instance }} has insufficient drives remaining. ({{ $value }})
            n25:
              alert: n25-NodeSoftwareRaidDiskFailure
              enabled: true
              expr: (node_md_disks{state="failed"} > 0)
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) MD RAID array {{ $labels.device }} on {{ $labels.instance }} needs attention. ({{ $value }})
            n26:
              alert: n26-NodeKernelVersionDeviations
              enabled: true
              expr: changes(node_uname_info[1h]) > 0
              for: 0m
              severity: info
              summary: (instance {{ $labels.instance }}) Kernel version for {{ $labels.instance }} has changed. ({{ $value }})
            n27:
              alert: n27-NodeOomKillDetected
              enabled: true
              expr: (increase(node_vmstat_oom_kill[1m]) > 0)
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) OOM kill detected ({{ $value }})
            n28:
              alert: n28-NodeEdacCorrectableErrorsDetected
              enabled: true
              expr: (increase(node_edac_correctable_errors_total[1m]) > 0)
              for: 0m
              severity: info
              summary: (instance {{ $labels.instance }}) Host {{ $labels.instance }} has had {{ printf %.0f $value }} correctable memory errors reported by EDAC in the last 5 minutes. ({{ $value }})
            n29:
              alert: n29-NodeEdacUncorrectableErrorsDetected
              enabled: true
              expr: (node_edac_uncorrectable_errors_total > 0)
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) Host {{ $labels.instance }} has had {{ printf %.0f $value }} uncorrectable memory errors reported by EDAC in the last 5 minutes. ({{ $value }})
            n30:
              alert: n30-NodeNetworkReceiveErrors
              enabled: true
              expr: (rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01)
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf %.0f $value }} receive errors in the last two minutes. ({{ $value }})
            n31:
              alert: n31-NodeNetworkTransmitErrors
              enabled: true
              expr: (rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01)
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf %.0f $value }} transmit errors in the last two minutes. ({{ $value }})
            n32:
              alert: n32-NodeNetworkBondDegraded
              enabled: true
              expr: ((node_bonding_active - node_bonding_slaves) != 0)
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Bond {{ $labels.device }} degraded on {{ $labels.instance }}. ({{ $value }})
            n33:
              alert: n33-NodeConntrackLimit
              enabled: true
              expr: (node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8)
              for: 5m
              severity: warning
              summary: (instance {{ $labels.instance }}) The number of conntrack is approaching limit ({{ $value }})
            n34:
              alert: n34-NodeClockSkew
              enabled: true
              expr: ((node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0))
              for: 10m
              severity: warning
              summary: (instance {{ $labels.instance }}) Clock skew detected. Clock is out of sync. Ensure NTP is configured correctly on this host. ({{ $value }})
            n35:
              alert: n35-NodeClockNotSynchronising
              enabled: true
              expr: (min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16)
              for: 2m
              severity: warning
              summary: (instance {{ $labels.instance }}) Clock not synchronising. Ensure NTP is configured on this host. ({{ $value }})
            n36:
              alert: n36-NodeRequiresReboot
              enabled: true
              expr: (node_reboot_required > 0)
              for: 4h
              severity: info
              summary: (instance {{ $labels.instance }}) {{ $labels.instance }} requires a reboot. ({{ $value }})
        prometheus:
          enabled: true
          rules:
            p01:
              alert: p01-PrometheusJobMissing
              enabled: true
              expr: absent(up{job="prometheus"})
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) A Prometheus job has disappeared ({{ $value }})
            p02:
              alert: p02-PrometheusTargetMissing
              enabled: true
              expr: up == 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) A Prometheus target has disappeared. An exporter might be crashed. ({{ $value }})
            p03:
              alert: p03-PrometheusAllTargetsMissing
              enabled: true
              expr: sum by (job) (up) == 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) A Prometheus job does not have living target anymore. ({{ $value }})
            p04:
              alert: p04-PrometheusTargetMissingWithWarmupTime
              enabled: true
              expr: sum by (instance, job) ((up == 0) * on (instance) group_left(__name__) (node_time_seconds - node_boot_time_seconds > 600))
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Allow a job time to start up (10 minutes) before alerting that it's down. ({{ $value }})
            p05:
              alert: p05-PrometheusConfigurationReloadFailure
              enabled: true
              expr: prometheus_config_last_reload_successful != 1
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) Prometheus configuration reload error ({{ $value }})
            p06:
              alert: p06-PrometheusTooManyRestarts
              enabled: true
              expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping. ({{ $value }})
            p07:
              alert: p07-PrometheusAlertmanagerJobMissing
              enabled: true
              expr: absent(up{job="alertmanager"})
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) A Prometheus AlertManager job has disappeared ({{ $value }})
            p08:
              alert: p08-PrometheusAlertmanagerConfigurationReloadFailure
              enabled: true
              expr: alertmanager_config_last_reload_successful != 1
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) AlertManager configuration reload error ({{ $value }})
            p09:
              alert: p09-PrometheusAlertmanagerConfigNotSynced
              enabled: true
              expr: count(count_values("config_hash", alertmanager_config_hash)) > 1
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) Configurations of AlertManager cluster instances are out of sync ({{ $value }})
            p10:
              alert: p10-PrometheusAlertmanagerE2eDeadManSwitch
              enabled: true
              expr: vector(1)
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager. ({{ $value }})
            p11:
              alert: p11-PrometheusNotConnectedToAlertmanager
              enabled: true
              expr: prometheus_notifications_alertmanagers_discovered < 1
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus cannot connect the alertmanager ({{ $value }})
            p12:
              alert: p12-PrometheusRuleEvaluationFailures
              enabled: true
              expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts. ({{ $value }})
            p13:
              alert: p13-PrometheusTemplateTextExpansionFailures
              enabled: true
              expr: increase(prometheus_template_text_expansion_failures_total[3m]) > 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus encountered {{ $value }} template text expansion failures ({{ $value }})
            p14:
              alert: p14-PrometheusRuleEvaluationSlow
              enabled: true
              expr: prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds
              for: 5m
              severity: warning
              summary: (instance {{ $labels.instance }}) Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query. ({{ $value }})
            p15:
              alert: p15-PrometheusNotificationsBacklog
              enabled: true
              expr: min_over_time(prometheus_notifications_queue_length[10m]) > 0
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) The Prometheus notification queue has not been empty for 10 minutes ({{ $value }})
            p16:
              alert: p16-PrometheusAlertmanagerNotificationFailing
              enabled: true
              expr: rate(alertmanager_notifications_failed_total[1m]) > 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Alertmanager is failing sending notifications ({{ $value }})
            p17:
              alert: p17-PrometheusTargetEmpty
              enabled: true
              expr: prometheus_sd_discovered_targets == 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus has no target in service discovery ({{ $value }})
            p18:
              alert: p18-PrometheusTargetScrapingSlow
              enabled: true
              expr: prometheus_target_interval_length_seconds{quantile="0.9"} / on (interval, instance, job) prometheus_target_interval_length_seconds{quantile="0.5"} > 1.05
              for: 5m
              severity: warning
              summary: (instance {{ $labels.instance }}) Prometheus is scraping exporters slowly since it exceeded the requested interval time. Your Prometheus server is under-provisioned. ({{ $value }})
            p19:
              alert: p19-PrometheusLargeScrape
              enabled: true
              expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10
              for: 5m
              severity: warning
              summary: (instance {{ $labels.instance }}) Prometheus has many scrapes that exceed the sample limit ({{ $value }})
            p20:
              alert: p20-PrometheusTargetScrapeDuplicate
              enabled: true
              expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) Prometheus has many samples rejected due to duplicate timestamps but different values ({{ $value }})
            p21:
              alert: p21-PrometheusTsdbCheckpointCreationFailures
              enabled: true
              expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus encountered {{ $value }} checkpoint creation failures ({{ $value }})
            p22:
              alert: p22-PrometheusTsdbCheckpointDeletionFailures
              enabled: true
              expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus encountered {{ $value }} checkpoint deletion failures ({{ $value }})
            p23:
              alert: p23-PrometheusTsdbCompactionsFailed
              enabled: true
              expr: increase(prometheus_tsdb_compactions_failed_total[1m]) > 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus encountered {{ $value }} TSDB compactions failures ({{ $value }})
            p24:
              alert: p24-PrometheusTsdbHeadTruncationsFailed
              enabled: true
              expr: increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus encountered {{ $value }} TSDB head truncation failures ({{ $value }})
            p25:
              alert: p25-PrometheusTsdbReloadFailures
              enabled: true
              expr: increase(prometheus_tsdb_reloads_failures_total[1m]) > 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus encountered {{ $value }} TSDB reload failures ({{ $value }})
            p26:
              alert: p26-PrometheusTsdbWalCorruptions
              enabled: true
              expr: increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus encountered {{ $value }} TSDB WAL corruptions ({{ $value }})
            p27:
              alert: p27-PrometheusTsdbWalTruncationsFailed
              enabled: true
              expr: increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0
              for: 0m
              severity: critical
              summary: (instance {{ $labels.instance }}) Prometheus encountered {{ $value }} TSDB WAL truncation failures ({{ $value }})
            p28:
              alert: p28-PrometheusTimeseriesCardinality
              enabled: true
              expr: label_replace(count by(__name__) ({__name__=~".+"}), "name", "$1", "__name__", "(.+)") > 10000
              for: 0m
              severity: warning
              summary: (instance {{ $labels.instance }}) The {{ $labels.name }} timeseries cardinality is getting very high {{ $value }} ({{ $value }})
        vcustom:
          enabled: true
          rules:
            v01:
              alert: v01-VentiPodCreated
              enabled: true
              expr: time() - (kube_pod_created{namespace="kube-system",pod=~".*-venti-.*"}) < 320
              for: 0m
              severity: info
              summary: Venti pod {{ $labels.namespace }}/{{ $labels.pod }} created within the last 5 minutes ({{ $value }})
            v02:
              alert: v02-Monday0000UTC
              enabled: true
              expr: day_of_week()==1 and vector(1)*time()%86400 < 140
              for: 0m
              severity: info
              summary: Alert triggered for Monday at 00:00 UTC ({{ $value }})
            v03:
              alert: v03-ReleasedPv
              enabled: true
              expr: kube_persistentvolume_status_phase{phase="Released"} == 1
              for: 1h
              severity: info
              summary: PersistentVolume {{ $labels.persistentvolume }} is in the Released state ({{ $value }})
            v04:
              alert: v04-TerminatingPod
              enabled: true
              expr: kube_pod_deletion_timestamp
              for: 1h
              severity: info
              summary: Pod {{ $labels.namespace }}/{{ $labels.pod }} is terminating ({{ $value }})
            v05:
              alert: v05-NodeDiskCount
              enabled: true
              expr: sum(node_disk_info{device=~"sd.|vd."}) by (node) > 23
              for: 5m
              severity: warning
              summary: Node {{ $labels.node }} has many disks ({{ $value }})
            v06:
              alert: v06-NodePodPressure
              enabled: true
              expr: kubelet_running_pods > 99
              for: 0m
              severity: warning
              summary: Node {{ $labels.instance }} is under pod pressure ({{ $value }})

      extraGroups: []
      # - name: example
      #   rules:
      #   - alert: HighRequestLatency
      #     expr: job:request_latency_seconds:mean5m{job="myjob"} > 0.5
      #     for: 10m
      #     severity: info
      #     summary: High request latency
    dashboards:
      useDefault: true
      extraDashboards: {}
      # cluster.yaml: |
      #   title: Cluster
      #   rows:
      #   - panels:
      #     - title: node
      #       type: piechart
      #       targets:
      #       - expr: sum(kube_node_status_condition{status="true"}) by (condition) > 0
      #         legend: "{{condition}}"

    datasources: []
    # - name: prometheus
    #   type: prometheus
    #   url: http://vs-prometheus-server
    # - name: lethe
    #   type: lethe
    #   url: http://vs-lethe
    discovery:
      enabled: false
    users: []
    # - username: admin
    #   hash: $2a$12$VcCDgh2NDk07JGN0rjGbM.Ad41qVR/YFJcgHp0UGns5JDymv..TOG
    #   isAdmin: true
  nodeSelector: {}
  hostAliases: []
  dnsConfig: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: []

  service:
    enabled: true
    annotations: {}
    labels: {}
    clusterIP: ""
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    sessionAffinity: None
    type: ClusterIP
  ingress:
    enabled: false
    annotations: {}
    extraLabels: {}
    hosts: []
    # - venti.domain.com
    tls: []
    # - secretName: chart-example-tls
    #   hosts:
    #     - venti.domain.com
    path: /
    pathType: Prefix
    extraPaths: []
