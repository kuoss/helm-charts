apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "venti.fullname" .}}-alertrules
  namespace: {{ template "venti-stack.namespace" .}}
data:
  # https://github.com/samber/awesome-prometheus-alerts/tree/ac09fd8a2d6459355fcaf93090a3b6a32575220f/dist/rules
  kubernetes.yml: |
    datasourceSelector:
      type: prometheus
      system: main
    groups:
    - name: Kubernetes
    
      rules:
    
        - alert: KubernetesNodeReady
          expr: 'kube_node_status_condition{condition="Ready",status="true"} == 0'
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes Node ready (instance {{`{{$labels.instance}}`}})
            description: "Node {{`{{$labels.node}}`}} has been unready for a long time\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesMemoryPressure
          expr: 'kube_node_status_condition{condition="MemoryPressure",status="true"} == 1'
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes memory pressure (instance {{`{{$labels.instance}}`}})
            description: "{{`{{$labels.node}}`}} has MemoryPressure condition\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesDiskPressure
          expr: 'kube_node_status_condition{condition="DiskPressure",status="true"} == 1'
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes disk pressure (instance {{`{{$labels.instance}}`}})
            description: "{{`{{$labels.node}}`}} has DiskPressure condition\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesNetworkUnavailable
          expr: 'kube_node_status_condition{condition="NetworkUnavailable",status="true"} == 1'
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes network unavailable (instance {{`{{$labels.instance}}`}})
            description: "{{`{{$labels.node}}`}} has NetworkUnavailable condition\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesOutOfCapacity
          expr: 'sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node) (kube_node_status_allocatable{resource="pods"}) * 100 > 90'
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes out of capacity (instance {{`{{$labels.instance}}`}})
            description: "{{`{{$labels.node}}`}} is out of capacity\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesContainerOomKiller
          expr: '(kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes container oom killer (instance {{`{{$labels.instance}}`}})
            description: "Container {{`{{$labels.container}}`}} in pod {{`{{$labels.namespace}}`}}/{{`{{$labels.pod}}`}} has been OOMKilled {{`{{$value}}`}} times in the last 10 minutes.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesJobFailed
          expr: 'kube_job_status_failed > 0'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes Job failed (instance {{`{{$labels.instance}}`}})
            description: "Job {{`{{$labels.namespace}}`}}/{{`{{$labels.exported_job}}`}} failed to complete\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesCronjobSuspended
          expr: 'kube_cronjob_spec_suspend != 0'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes CronJob suspended (instance {{`{{$labels.instance}}`}})
            description: "CronJob {{`{{$labels.namespace}}`}}/{{`{{$labels.cronjob}}`}} is suspended\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesPersistentvolumeclaimPending
          expr: 'kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1'
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes PersistentVolumeClaim pending (instance {{`{{$labels.instance}}`}})
            description: "PersistentVolumeClaim {{`{{$labels.namespace}}`}}/{{`{{$labels.persistentvolumeclaim}}`}} is pending\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesVolumeOutOfDiskSpace
          expr: 'kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10'
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes Volume out of disk space (instance {{`{{$labels.instance}}`}})
            description: "Volume is almost full (< 10% left)\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesVolumeFullInFourDays
          expr: 'predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) < 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes Volume full in four days (instance {{`{{$labels.instance}}`}})
            description: "{{`{{$labels.namespace}}`}}/{{`{{$labels.persistentvolumeclaim}}`}} is expected to fill up within four days. Currently {{`{{$value | humanize}}`}}% is available.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesPersistentvolumeError
          expr: 'kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes PersistentVolume error (instance {{`{{$labels.instance}}`}})
            description: "Persistent volume is in bad state\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesStatefulsetDown
          expr: 'kube_statefulset_replicas != kube_statefulset_status_replicas_ready > 0'
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes StatefulSet down (instance {{`{{$labels.instance}}`}})
            description: "A StatefulSet went down\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesHpaScalingAbility
          expr: 'kube_horizontalpodautoscaler_status_condition{status="false", condition="AbleToScale"} == 1'
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes HPA scaling ability (instance {{`{{$labels.instance}}`}})
            description: "Pod is unable to scale\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesHpaMetricAvailability
          expr: 'kube_horizontalpodautoscaler_status_condition{status="false", condition="ScalingActive"} == 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes HPA metric availability (instance {{`{{$labels.instance}}`}})
            description: "HPA is not able to collect metrics\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesHpaScaleCapability
          expr: 'kube_horizontalpodautoscaler_status_desired_replicas >= kube_horizontalpodautoscaler_spec_max_replicas'
          for: 2m
          labels:
            severity: info
          annotations:
            summary: Kubernetes HPA scale capability (instance {{`{{$labels.instance}}`}})
            description: "The maximum number of desired Pods has been hit\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesHpaUnderutilized
          expr: 'max(quantile_over_time(0.5, kube_horizontalpodautoscaler_status_desired_replicas[1d]) == kube_horizontalpodautoscaler_spec_min_replicas) by (horizontalpodautoscaler) > 3'
          for: 0m
          labels:
            severity: info
          annotations:
            summary: Kubernetes HPA underutilized (instance {{`{{$labels.instance}}`}})
            description: "HPA is constantly at minimum replicas for 50% of the time. Potential cost saving here.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesPodNotHealthy
          expr: 'sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0'
          for: 15m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes Pod not healthy (instance {{`{{$labels.instance}}`}})
            description: "Pod has been in a non-ready state for longer than 15 minutes.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesPodCrashLooping
          expr: 'increase(kube_pod_container_status_restarts_total[1m]) > 3'
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes pod crash looping (instance {{`{{$labels.instance}}`}})
            description: "Pod {{`{{$labels.pod}}`}} is crash looping\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesReplicassetMismatch
          expr: 'kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas'
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes ReplicasSet mismatch (instance {{`{{$labels.instance}}`}})
            description: "Deployment Replicas mismatch\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesDeploymentReplicasMismatch
          expr: 'kube_deployment_spec_replicas != kube_deployment_status_replicas_available'
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes Deployment replicas mismatch (instance {{`{{$labels.instance}}`}})
            description: "Deployment Replicas mismatch\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesStatefulsetReplicasMismatch
          expr: 'kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas'
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes StatefulSet replicas mismatch (instance {{`{{$labels.instance}}`}})
            description: "A StatefulSet does not match the expected number of replicas.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesDeploymentGenerationMismatch
          expr: 'kube_deployment_status_observed_generation != kube_deployment_metadata_generation'
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes Deployment generation mismatch (instance {{`{{$labels.instance}}`}})
            description: "A Deployment has failed but has not been rolled back.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesStatefulsetGenerationMismatch
          expr: 'kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation'
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes StatefulSet generation mismatch (instance {{`{{$labels.instance}}`}})
            description: "A StatefulSet has failed but has not been rolled back.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesStatefulsetUpdateNotRolledOut
          expr: 'max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)'
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes StatefulSet update not rolled out (instance {{`{{$labels.instance}}`}})
            description: "StatefulSet update has not been rolled out.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesDaemonsetRolloutStuck
          expr: 'kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0'
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes DaemonSet rollout stuck (instance {{`{{$labels.instance}}`}})
            description: "Some Pods of DaemonSet are not scheduled or not ready\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesDaemonsetMisscheduled
          expr: 'kube_daemonset_status_number_misscheduled > 0'
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes DaemonSet misscheduled (instance {{`{{$labels.instance}}`}})
            description: "Some DaemonSet Pods are running where they are not supposed to run\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesCronjobTooLong
          expr: 'time() - kube_cronjob_next_schedule_time > 3600'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes CronJob too long (instance {{`{{$labels.instance}}`}})
            description: "CronJob {{`{{$labels.namespace}}`}}/{{`{{$labels.cronjob}}`}} is taking more than 1h to complete.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesJobSlowCompletion
          expr: 'kube_job_spec_completions - kube_job_status_succeeded > 0'
          for: 12h
          labels:
            severity: critical
          annotations:
            summary: Kubernetes job slow completion (instance {{`{{$labels.instance}}`}})
            description: "Kubernetes Job {{`{{$labels.namespace}}`}}/{{`{{$labels.job_name}}`}} did not complete in time.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesApiServerErrors
          expr: 'sum(rate(apiserver_request_total{job="apiserver",code=~"^(?:5..)$"}[1m])) / sum(rate(apiserver_request_total{job="apiserver"}[1m])) * 100 > 3'
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes API server errors (instance {{`{{$labels.instance}}`}})
            description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesApiClientErrors
          expr: '(sum(rate(rest_client_requests_total{code=~"(4|5).."}[1m])) by (instance, job) / sum(rate(rest_client_requests_total[1m])) by (instance, job)) * 100 > 1'
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes API client errors (instance {{`{{$labels.instance}}`}})
            description: "Kubernetes API client is experiencing high error rate\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesClientCertificateExpiresNextWeek
          expr: 'apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes client certificate expires next week (instance {{`{{$labels.instance}}`}})
            description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesClientCertificateExpiresSoon
          expr: 'apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 24*60*60'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Kubernetes client certificate expires soon (instance {{`{{$labels.instance}}`}})
            description: "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
    
        - alert: KubernetesApiServerLatency
          expr: 'histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{subresource!="log",verb!~"^(?:CONNECT|WATCHLIST|WATCH|PROXY)$"} [10m])) WITHOUT (instance, resource)) / 1e+06 > 1'
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: Kubernetes API server latency (instance {{`{{$labels.instance}}`}})
            description: "Kubernetes API server has a 99th percentile latency of {{`{{$value}}`}} seconds for {{`{{$labels.verb}}`}} {{`{{$labels.resource}}`}}.\n  VALUE = {{`{{$value}}`}}\n  LABELS = {{`{{$labels}}`}}"
  prometheus.yml: |
    datasourceSelector:
      type: prometheus
      system: main
    groups:
    - name: Prometheus
    
      rules:
    
        - alert: PrometheusJobMissing
          expr: 'absent(up{job="prometheus"})'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus job missing (instance {{`{{ $labels.instance }}`}})
            description: "A Prometheus job has disappeared\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTargetMissing
          expr: 'up == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus target missing (instance {{`{{ $labels.instance }}`}})
            description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusAllTargetsMissing
          expr: 'sum by (job) (up) == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus all targets missing (instance {{`{{ $labels.instance }}`}})
            description: "A Prometheus job does not have living target anymore.\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTargetMissingWithWarmupTime
          expr: 'sum by (instance, job) ((up == 0) * on (instance) group_right(job) (node_time_seconds - node_boot_time_seconds > 600))'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus target missing with warmup time (instance {{`{{ $labels.instance }}`}})
            description: "Allow a job time to start up (10 minutes) before alerting that it's down.\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusConfigurationReloadFailure
          expr: 'prometheus_config_last_reload_successful != 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus configuration reload failure (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus configuration reload error\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTooManyRestarts
          expr: 'changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus too many restarts (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusAlertmanagerJobMissing
          expr: 'absent(up{job="alertmanager"})'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus AlertManager job missing (instance {{`{{ $labels.instance }}`}})
            description: "A Prometheus AlertManager job has disappeared\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusAlertmanagerConfigurationReloadFailure
          expr: 'alertmanager_config_last_reload_successful != 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus AlertManager configuration reload failure (instance {{`{{ $labels.instance }}`}})
            description: "AlertManager configuration reload error\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusAlertmanagerConfigNotSynced
          expr: 'count(count_values("config_hash", alertmanager_config_hash)) > 1'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus AlertManager config not synced (instance {{`{{ $labels.instance }}`}})
            description: "Configurations of AlertManager cluster instances are out of sync\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusAlertmanagerE2eDeadManSwitch
          expr: 'vector(1)'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus AlertManager E2E dead man switch (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager.\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusNotConnectedToAlertmanager
          expr: 'prometheus_notifications_alertmanagers_discovered < 1'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus not connected to alertmanager (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus cannot connect the alertmanager\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusRuleEvaluationFailures
          expr: 'increase(prometheus_rule_evaluation_failures_total[3m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus rule evaluation failures (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus encountered {{`{{ $value }}`}} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTemplateTextExpansionFailures
          expr: 'increase(prometheus_template_text_expansion_failures_total[3m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus template text expansion failures (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus encountered {{`{{ $value }}`}} template text expansion failures\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusRuleEvaluationSlow
          expr: 'prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Prometheus rule evaluation slow (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query.\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusNotificationsBacklog
          expr: 'min_over_time(prometheus_notifications_queue_length[10m]) > 0'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus notifications backlog (instance {{`{{ $labels.instance }}`}})
            description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusAlertmanagerNotificationFailing
          expr: 'rate(alertmanager_notifications_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus AlertManager notification failing (instance {{`{{ $labels.instance }}`}})
            description: "Alertmanager is failing sending notifications\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTargetEmpty
          expr: 'prometheus_sd_discovered_targets == 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus target empty (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus has no target in service discovery\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTargetScrapingSlow
          expr: 'prometheus_target_interval_length_seconds{quantile="0.9"} / on (interval, instance, job) prometheus_target_interval_length_seconds{quantile="0.5"} > 1.05'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Prometheus target scraping slow (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus is scraping exporters slowly since it exceeded the requested interval time. Your Prometheus server is under-provisioned.\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusLargeScrape
          expr: 'increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10'
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Prometheus large scrape (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTargetScrapeDuplicate
          expr: 'increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus target scrape duplicate (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus has many samples rejected due to duplicate timestamps but different values\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTsdbCheckpointCreationFailures
          expr: 'increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB checkpoint creation failures (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus encountered {{`{{ $value }}`}} checkpoint creation failures\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTsdbCheckpointDeletionFailures
          expr: 'increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB checkpoint deletion failures (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus encountered {{`{{ $value }}`}} checkpoint deletion failures\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTsdbCompactionsFailed
          expr: 'increase(prometheus_tsdb_compactions_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB compactions failed (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus encountered {{`{{ $value }}`}} TSDB compactions failures\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTsdbHeadTruncationsFailed
          expr: 'increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB head truncations failed (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus encountered {{`{{ $value }}`}} TSDB head truncation failures\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTsdbReloadFailures
          expr: 'increase(prometheus_tsdb_reloads_failures_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB reload failures (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus encountered {{`{{ $value }}`}} TSDB reload failures\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTsdbWalCorruptions
          expr: 'increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB WAL corruptions (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus encountered {{`{{ $value }}`}} TSDB WAL corruptions\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTsdbWalTruncationsFailed
          expr: 'increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0'
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Prometheus TSDB WAL truncations failed (instance {{`{{ $labels.instance }}`}})
            description: "Prometheus encountered {{`{{ $value }}`}} TSDB WAL truncation failures\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    
        - alert: PrometheusTimeserieCardinality
          expr: 'label_replace(count by(__name__) ({__name__=~".+"}), "name", "$1", "__name__", "(.+)") > 10000'
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Prometheus timeserie cardinality (instance {{`{{ $labels.instance }}`}})
            description: "The \"{{`{{ $labels.name }}`}}\" timeserie cardinality is getting very high: {{`{{ $value }}`}}\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"
    